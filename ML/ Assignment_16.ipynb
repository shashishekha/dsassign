{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b9875f",
   "metadata": {},
   "source": [
    "### 1. In a linear equation, what is the difference between a dependent variable and an independent\n",
    "### variable?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40157a57",
   "metadata": {},
   "source": [
    "<b>ans:</b>Algebraically, a linear equation typically takes the form y = mx + b, where m and b are constants, x is the independent variable, y is the dependent variable. The slope tells us how the dependent variable (y) changes for every one unit increase in the independent (x) variable, on average.\n",
    "\n",
    "The variables in a study of a cause-and-effect relationship are called the independent and dependent variables. The independent variable is the cause. Its value is independent of other variables in your study. The dependent variable is the effect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e17afa9",
   "metadata": {},
   "source": [
    "### 2. What is the concept of simple linear regression? Give a specific example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf54747",
   "metadata": {},
   "source": [
    "<b>ans : </b> Simple linear regression is a regression model that estimates the relationship between one independent variable and one dependent variable using a straight line. Both variables should be quantitative. Linear regression most often uses mean-square error (MSE) to calculate the error of the model.\n",
    "\n",
    "For example, suppose that height was the only determinant of body weight.In this example, if an individual was 70 inches tall, we would predict his weight to be: Weight = 80 + 2 x (70) = 220 lbs. In this simple linear regression, we are examining the impact of one independent variable on the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e27a45",
   "metadata": {},
   "source": [
    "### 3. In a linear regression, define the slope.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c9ae31",
   "metadata": {},
   "source": [
    "<b>ans : </b> The slope of a regression line (b) represents the rate of change in y as x changes. Because y is dependent on x, the slope describes the predicted values of y given x. The slope must be calculated before the y-intercept when using a linear regression, as the intercept is calculated using the slope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0fa934",
   "metadata": {},
   "source": [
    "### 4. Determine the graph&#39;s slope, where the lower point on the line is represented as (3, 2) and the\n",
    "### higher point is represented as (2, 2).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b715d35d",
   "metadata": {},
   "source": [
    "<b>ans : </b> The graph's slope, where the lower point on the line is represented as (3, 2) and the higher point is represented as (2, 2) is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2f4e84",
   "metadata": {},
   "source": [
    "### 5. In linear regression, what are the conditions for a positive slope?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e802dc9",
   "metadata": {},
   "source": [
    "<b>ans : </b> If the slope is positive, y increases as x increases, and the function runs \"uphill\" (going left to right). If the slope is zero, y does not change, thus is constant—a horizontal line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b0f0b8",
   "metadata": {},
   "source": [
    "### 6. In linear regression, what are the conditions for a negative slope?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b073507",
   "metadata": {},
   "source": [
    "<b>ans : </b> If the slope is negative, y decreases as x increases and the function runs downhill."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a75fe7",
   "metadata": {},
   "source": [
    "### 7. What is multiple linear regression and how does it work?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ba3af",
   "metadata": {},
   "source": [
    "<b>ans : </b> Multiple linear regression refers to a statistical technique that uses two or more independent variables to predict the outcome of a dependent variable. The technique enables analysts to determine the variation of the model and the relative contribution of each independent variable in the total variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6936b322",
   "metadata": {},
   "source": [
    "### 8. In multiple linear regression, define the number of squares due to error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c6409f",
   "metadata": {},
   "source": [
    "<b>ans : </b> The mean squared error (MSE) tells you how close a regression line is to a set of points. It does this by taking the distances from the points to the regression line (these distances are the “errors”) and squaring them. It's called the mean squared error as you're finding the average of a set of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a143cbc3",
   "metadata": {},
   "source": [
    "### 9. In multiple linear regression, define the number of squares due to regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2096f8",
   "metadata": {},
   "source": [
    "<b>ans : </b> Sum of squares is a statistical technique used in regression analysis to determine the dispersion of data points. In a regression analysis, the goal is to determine how well a data series can be fitted to a function that might help to explain how the data series was generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ee47a",
   "metadata": {},
   "source": [
    "\n",
    "### In a regression equation, what is multicollinearity?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162029d3",
   "metadata": {},
   "source": [
    "<b>ans : </b> Multicollinearity occurs when two or more independent variables are highly correlated with one another in a regression model. This means that an independent variable can be predicted from another independent variable in a regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81feb9e0",
   "metadata": {},
   "source": [
    "### 11. What is heteroskedasticity, and what does it mean?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec717029",
   "metadata": {},
   "source": [
    "<b>ans : </b> Heteroskedasticity (also spelled heteroscedasticity) refers to the error variance, or dependence of scattering, within a minimum of one independent variable within a particular sample.A common cause of variances outside the minimum requirement is often attributed to issues of data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15a37be",
   "metadata": {},
   "source": [
    "### 12. Describe the concept of ridge regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec0e8f0",
   "metadata": {},
   "source": [
    "<b>ans : </b> Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity.By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors. It is hoped that the net effect will be to give estimates that are more reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c58937",
   "metadata": {},
   "source": [
    "### 13. Describe the concept of lasso regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5965872b",
   "metadata": {},
   "source": [
    "<b>ans : </b> In statistics and machine learning, lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the resulting statistical model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ddd6b7",
   "metadata": {},
   "source": [
    "### 14. What is polynomial regression and how does it work?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a64c4f",
   "metadata": {},
   "source": [
    "Polynomial Regression is a form of Linear regression known as a special case of Multiple linear regression which estimates the relationship as an nth degree polynomial. Polynomial Regression is sensitive to outliers so the presence of one or two outliers can also badly affect the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2380a64b",
   "metadata": {},
   "source": [
    "### 15. Describe the basis function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f5695c",
   "metadata": {},
   "source": [
    "This is a generalization of linear regression that essentially replaces each input with a function of the input. (A linear basis function model that uses the identity function is just linear regression.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8a9c02",
   "metadata": {},
   "source": [
    "\n",
    "### 16. Describe how logistic regression works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5947509",
   "metadata": {},
   "source": [
    "Logistic regression uses an equation as the representation, very much like linear regression. Input values (x) are combined linearly using weights or coefficient values (referred to as the Greek capital letter Beta) to predict an output value (y)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
